{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform PCA, 2, 4, 8, 20 Components\n",
    "# pca_2= PCA(n_components=2)\n",
    "# principal_components_train_2 = pca_2.fit_transform(X_train_std)\n",
    "\n",
    "# pca_8 = PCA(n_components=8)\n",
    "# principal_components_train_8 = pca_8.fit_transform(X_train_std)\n",
    "\n",
    "# pca_20 = PCA(n_components=20)\n",
    "# principal_components_train_20 = pca_20.fit_transform(X_train_std)\n",
    "\n",
    "# pca_40 = PCA(n_components=40)\n",
    "# principal_components_train_40 = pca_40.fit_transform(X_train_std)\n",
    "\n",
    "# # Determine optimal number of clusters (k)\n",
    "# num_clusters = 6\n",
    "\n",
    "# # Apply K-Means clustering\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# clusters_train_2 = kmeans.fit_predict(principal_components_train_2)\n",
    "# clusters_train_8 = kmeans.fit_predict(principal_components_train_8)\n",
    "# clusters_train_20 = kmeans.fit_predict(principal_components_train_20)\n",
    "# clusters_train_40 = kmeans.fit_predict(principal_components_train_40)\n",
    "\n",
    "# plt.subplot(2, 2, 1)\n",
    "# scatter = plt.scatter(principal_components_train_2[:, 0], principal_components_train_2[:, 1], c=clusters_train_2, cmap='viridis')\n",
    "# plt.xlabel('PC1')\n",
    "# plt.ylabel('PC2')\n",
    "# plt.title('PCA Space Clusters (2 Components)')\n",
    "# plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# plt.subplot(2, 2, 2)\n",
    "# scatter = plt.scatter(principal_components_train_8[:, 0], principal_components_train_8[:, 1], c=clusters_train_8, cmap='viridis')\n",
    "# plt.xlabel('PC1')\n",
    "# plt.ylabel('PC2')\n",
    "# plt.title('PCA Space Clusters (8 Components)')\n",
    "# plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# scatter = plt.scatter(principal_components_train_20[:, 0], principal_components_train_20[:, 1], c=clusters_train_20, cmap='viridis')\n",
    "# plt.xlabel('PC1')\n",
    "# plt.ylabel('PC2')\n",
    "# plt.title('PCA Space Clusters (20 Components)')\n",
    "# plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# plt.subplot(2, 2, 4)\n",
    "# scatter = plt.scatter(principal_components_train_40[:, 0], principal_components_train_40[:, 1], c=clusters_train_40, cmap='viridis')\n",
    "# plt.xlabel('PC1')\n",
    "# plt.ylabel('PC2')\n",
    "# plt.title('PCA Space Clusters (40 Components)')\n",
    "# plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Applying LDA on PCA Components\n",
    "1. Performing LDA\n",
    "2. Plotting LDA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LDA(n_components=num_clusters - 1)\n",
    "\n",
    "# Fit LDA using PCA components and cluster labels\n",
    "lda_components_train = lda.fit_transform(principal_components_train, clusters_train)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "lda_df = pd.DataFrame(lda_components_train, columns=[f'LD{i+1}' for i in range(lda_components_train.shape[1])])\n",
    "lda_df['Cluster'] = clusters_train\n",
    "lda_df['SampleID'] = samples_train\n",
    "\n",
    "# Plot first two LDA components\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(lda_components_train[:, 0], lda_components_train[:, 1], c=clusters_train, cmap='viridis')\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.title('LDA of PCA Components')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for the NN\n",
    "1. Use LDA Components as Features\n",
    "2. Split Data into Training and Testing Sets\n",
    "3. Convert Labels to Categorical Format\n",
    "\n",
    "## Explanation\n",
    "Features: Using LDA components leverages the maximized class separation.\n",
    "\n",
    "Data Splitting: Ensures model is evaluated on unseen data.\n",
    "\n",
    "One-Hot Encoding: Converts integer labels to categorical format for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the test data using the scaler fitted on training data\n",
    "X_test_std = scaler.transform(X_test_raw)\n",
    "\n",
    "# Apply PCA transformation using the PCA fitted on training data\n",
    "principal_components_test = pca.transform(X_test_std)\n",
    "\n",
    "# Note: We do NOT perform clustering on test data\n",
    "# Instead, we will use the neural network to predict cluster labels\n",
    "\n",
    "# Apply LDA transformation using the LDA fitted on training data\n",
    "lda_components_test = lda.transform(principal_components_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare Data for Neural Network\n",
    "# Features: LDA components\n",
    "X_train = lda_components_train\n",
    "X_test = lda_components_test\n",
    "\n",
    "# Targets: Cluster labels for training data\n",
    "y_train = clusters_train\n",
    "\n",
    "# Since we don't have cluster labels for test data, we will predict them using the neural network\n",
    "# For evaluation purposes, we can use the known clusters if we had them, but in this case, we don't\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Convert training labels to categorical format\n",
    "num_classes = num_clusters\n",
    "y_train_cat = to_categorical(y_train, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training the Neural Network\n",
    "1. Building the Neural Network\n",
    "2. Compiling the Model\n",
    "3. Training the Model\n",
    "\n",
    "## Explanation\n",
    "### Model Architecture:\n",
    "- Input Layer: Size matches the number of LDA components.\n",
    "- Hidden Layers: Include dense layers with ReLU activation.\n",
    "- Output Layer: Uses softmax activation for multi-class classification.\n",
    "### Compilation:\n",
    "- Optimizer: Adam optimizer for efficient training.\n",
    "- Loss Function: Categorical crossentropy for multi-class classification.\n",
    "- Metrics: Accuracy to evaluate performance.\n",
    "- Training Parameters:\n",
    "- Epochs: Number of times the model sees the entire dataset.\n",
    "- Validation Split: Fraction of training data used for validation.\n",
    "- Batch Size: Number of samples processed before updating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_cat,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=5,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "1. Evaluate on Test Set\n",
    "2. Make Predictions\n",
    "3. Classification Report\n",
    "4. Confusion Matrix\n",
    "\n",
    "## Explanation\n",
    "Test Accuracy: Provides an overall performance metric on unseen data.\n",
    "\n",
    "Classification Report: Includes precision, recall, f1-score for each class.\n",
    "\n",
    "Confusion Matrix: Visualizes the model's performance in terms of correct and incorrect classifications for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Make Predictions on Test Data\n",
    "# Predict cluster probabilities for test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Assign Cluster Labels to Test Data Based on Nearest Centroid\n",
    "# Compute cluster centroids in LDA space for training data\n",
    "lda_centroids = []\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = lda_components_train[y_train == i]\n",
    "    centroid = np.mean(cluster_data, axis=0)\n",
    "    lda_centroids.append(centroid)\n",
    "lda_centroids = np.array(lda_centroids)\n",
    "\n",
    "# Assign cluster labels to test data based on nearest centroid in LDA space\n",
    "distances = cdist(lda_components_test, lda_centroids)\n",
    "y_test_assigned = np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Generate Confusion Matrix and Classification Report\n",
    "cm = confusion_matrix(y_test_assigned, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "plt.xlabel('Predicted Cluster')\n",
    "plt.ylabel('Assigned Cluster')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_assigned, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
